{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 120\n",
      "Tamaño del conjunto de validación: 30\n"
     ]
    }
   ],
   "source": [
    "# Importando las bibliotecas necesarias\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape[0]}\")\n",
    "print(f\"Tamaño del conjunto de validación: {X_val.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleFeedForwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFeedForwardNN, self).__init__()\n",
    "        \n",
    "        # Capa de entrada\n",
    "        self.input_layer = nn.Linear(4, 10)  # 4 características de entrada y 10 neuronas en la capa oculta\n",
    "        \n",
    "        # Capa oculta\n",
    "        self.hidden_layer = nn.Linear(10, 10)  # 10 neuronas en la capa oculta y 10 en la siguiente\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.output_layer = nn.Linear(10, 3)  # 10 neuronas en la capa oculta y 3 neuronas de salida\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pasar la entrada a través de la capa de entrada y luego aplicar ReLU\n",
    "        x = nn.ReLU()(self.input_layer(x))\n",
    "        \n",
    "        # Pasar la salida anterior a través de la capa oculta y luego aplicar ReLU\n",
    "        x = nn.ReLU()(self.hidden_layer(x))\n",
    "        \n",
    "        # Pasar la salida anterior a través de la capa de salida\n",
    "        # Nota: No aplicamos Softmax aquí porque durante el entrenamiento utilizaremos\n",
    "        # CrossEntropyLoss que ya aplica Softmax.\n",
    "        return self.output_layer(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [CrossEntropy] - Training Loss: 1.0722, Test Loss: 1.0456\n",
      "Epoch 2 [CrossEntropy] - Training Loss: 1.0607, Test Loss: 1.0338\n",
      "Epoch 3 [CrossEntropy] - Training Loss: 1.0480, Test Loss: 1.0218\n",
      "Epoch 4 [CrossEntropy] - Training Loss: 1.0366, Test Loss: 1.0103\n",
      "Epoch 5 [CrossEntropy] - Training Loss: 1.0261, Test Loss: 0.9987\n",
      "Epoch 6 [CrossEntropy] - Training Loss: 1.0166, Test Loss: 0.9871\n",
      "Epoch 7 [CrossEntropy] - Training Loss: 1.0039, Test Loss: 0.9750\n",
      "Epoch 8 [CrossEntropy] - Training Loss: 0.9919, Test Loss: 0.9630\n",
      "Epoch 9 [CrossEntropy] - Training Loss: 0.9821, Test Loss: 0.9507\n",
      "Epoch 10 [CrossEntropy] - Training Loss: 0.9718, Test Loss: 0.9377\n",
      "Epoch 11 [CrossEntropy] - Training Loss: 0.9601, Test Loss: 0.9241\n",
      "Epoch 12 [CrossEntropy] - Training Loss: 0.9458, Test Loss: 0.9100\n",
      "Epoch 13 [CrossEntropy] - Training Loss: 0.9346, Test Loss: 0.8954\n",
      "Epoch 14 [CrossEntropy] - Training Loss: 0.9186, Test Loss: 0.8804\n",
      "Epoch 15 [CrossEntropy] - Training Loss: 0.9078, Test Loss: 0.8646\n",
      "Epoch 16 [CrossEntropy] - Training Loss: 0.8904, Test Loss: 0.8483\n",
      "Epoch 17 [CrossEntropy] - Training Loss: 0.8815, Test Loss: 0.8315\n",
      "Epoch 18 [CrossEntropy] - Training Loss: 0.8662, Test Loss: 0.8145\n",
      "Epoch 19 [CrossEntropy] - Training Loss: 0.8455, Test Loss: 0.7976\n",
      "Epoch 20 [CrossEntropy] - Training Loss: 0.8276, Test Loss: 0.7799\n",
      "Epoch 21 [CrossEntropy] - Training Loss: 0.8154, Test Loss: 0.7618\n",
      "Epoch 22 [CrossEntropy] - Training Loss: 0.8006, Test Loss: 0.7433\n",
      "Epoch 23 [CrossEntropy] - Training Loss: 0.7770, Test Loss: 0.7250\n",
      "Epoch 24 [CrossEntropy] - Training Loss: 0.7581, Test Loss: 0.7066\n",
      "Epoch 25 [CrossEntropy] - Training Loss: 0.7508, Test Loss: 0.6879\n",
      "Epoch 26 [CrossEntropy] - Training Loss: 0.7303, Test Loss: 0.6696\n",
      "Epoch 27 [CrossEntropy] - Training Loss: 0.7163, Test Loss: 0.6516\n",
      "Epoch 28 [CrossEntropy] - Training Loss: 0.6991, Test Loss: 0.6340\n",
      "Epoch 29 [CrossEntropy] - Training Loss: 0.6830, Test Loss: 0.6167\n",
      "Epoch 30 [CrossEntropy] - Training Loss: 0.6673, Test Loss: 0.5997\n",
      "Epoch 31 [CrossEntropy] - Training Loss: 0.6418, Test Loss: 0.5838\n",
      "Epoch 32 [CrossEntropy] - Training Loss: 0.6307, Test Loss: 0.5679\n",
      "Epoch 33 [CrossEntropy] - Training Loss: 0.6190, Test Loss: 0.5525\n",
      "Epoch 34 [CrossEntropy] - Training Loss: 0.5963, Test Loss: 0.5380\n",
      "Epoch 35 [CrossEntropy] - Training Loss: 0.5868, Test Loss: 0.5241\n",
      "Epoch 36 [CrossEntropy] - Training Loss: 0.5701, Test Loss: 0.5107\n",
      "Epoch 37 [CrossEntropy] - Training Loss: 0.5656, Test Loss: 0.4980\n",
      "Epoch 38 [CrossEntropy] - Training Loss: 0.5532, Test Loss: 0.4860\n",
      "Epoch 39 [CrossEntropy] - Training Loss: 0.5439, Test Loss: 0.4748\n",
      "Epoch 40 [CrossEntropy] - Training Loss: 0.5254, Test Loss: 0.4643\n",
      "Epoch 41 [CrossEntropy] - Training Loss: 0.5217, Test Loss: 0.4544\n",
      "Epoch 42 [CrossEntropy] - Training Loss: 0.5049, Test Loss: 0.4450\n",
      "Epoch 43 [CrossEntropy] - Training Loss: 0.5017, Test Loss: 0.4360\n",
      "Epoch 44 [CrossEntropy] - Training Loss: 0.4914, Test Loss: 0.4274\n",
      "Epoch 45 [CrossEntropy] - Training Loss: 0.4857, Test Loss: 0.4193\n",
      "Epoch 46 [CrossEntropy] - Training Loss: 0.4766, Test Loss: 0.4117\n",
      "Epoch 47 [CrossEntropy] - Training Loss: 0.4555, Test Loss: 0.4043\n",
      "Epoch 48 [CrossEntropy] - Training Loss: 0.4591, Test Loss: 0.3973\n",
      "Epoch 49 [CrossEntropy] - Training Loss: 0.4582, Test Loss: 0.3905\n",
      "Epoch 50 [CrossEntropy] - Training Loss: 0.4423, Test Loss: 0.3841\n",
      "Epoch 1 [MSELoss] - Training Loss: 0.3817, Test Loss: 0.3741\n",
      "Epoch 2 [MSELoss] - Training Loss: 0.3655, Test Loss: 0.3519\n",
      "Epoch 3 [MSELoss] - Training Loss: 0.3400, Test Loss: 0.3268\n",
      "Epoch 4 [MSELoss] - Training Loss: 0.3131, Test Loss: 0.3019\n",
      "Epoch 5 [MSELoss] - Training Loss: 0.2904, Test Loss: 0.2782\n",
      "Epoch 6 [MSELoss] - Training Loss: 0.2676, Test Loss: 0.2564\n",
      "Epoch 7 [MSELoss] - Training Loss: 0.2468, Test Loss: 0.2366\n",
      "Epoch 8 [MSELoss] - Training Loss: 0.2293, Test Loss: 0.2179\n",
      "Epoch 9 [MSELoss] - Training Loss: 0.2127, Test Loss: 0.2007\n",
      "Epoch 10 [MSELoss] - Training Loss: 0.1958, Test Loss: 0.1848\n",
      "Epoch 11 [MSELoss] - Training Loss: 0.1816, Test Loss: 0.1702\n",
      "Epoch 12 [MSELoss] - Training Loss: 0.1683, Test Loss: 0.1563\n",
      "Epoch 13 [MSELoss] - Training Loss: 0.1563, Test Loss: 0.1438\n",
      "Epoch 14 [MSELoss] - Training Loss: 0.1444, Test Loss: 0.1323\n",
      "Epoch 15 [MSELoss] - Training Loss: 0.1335, Test Loss: 0.1220\n",
      "Epoch 16 [MSELoss] - Training Loss: 0.1237, Test Loss: 0.1127\n",
      "Epoch 17 [MSELoss] - Training Loss: 0.1164, Test Loss: 0.1043\n",
      "Epoch 18 [MSELoss] - Training Loss: 0.1094, Test Loss: 0.0969\n",
      "Epoch 19 [MSELoss] - Training Loss: 0.1020, Test Loss: 0.0906\n",
      "Epoch 20 [MSELoss] - Training Loss: 0.0957, Test Loss: 0.0852\n",
      "Epoch 21 [MSELoss] - Training Loss: 0.0924, Test Loss: 0.0805\n",
      "Epoch 22 [MSELoss] - Training Loss: 0.0882, Test Loss: 0.0765\n",
      "Epoch 23 [MSELoss] - Training Loss: 0.0854, Test Loss: 0.0732\n",
      "Epoch 24 [MSELoss] - Training Loss: 0.0821, Test Loss: 0.0705\n",
      "Epoch 25 [MSELoss] - Training Loss: 0.0806, Test Loss: 0.0682\n",
      "Epoch 26 [MSELoss] - Training Loss: 0.0784, Test Loss: 0.0662\n",
      "Epoch 27 [MSELoss] - Training Loss: 0.0762, Test Loss: 0.0645\n",
      "Epoch 28 [MSELoss] - Training Loss: 0.0738, Test Loss: 0.0631\n",
      "Epoch 29 [MSELoss] - Training Loss: 0.0746, Test Loss: 0.0617\n",
      "Epoch 30 [MSELoss] - Training Loss: 0.0728, Test Loss: 0.0604\n",
      "Epoch 31 [MSELoss] - Training Loss: 0.0718, Test Loss: 0.0593\n",
      "Epoch 32 [MSELoss] - Training Loss: 0.0683, Test Loss: 0.0582\n",
      "Epoch 33 [MSELoss] - Training Loss: 0.0704, Test Loss: 0.0572\n",
      "Epoch 34 [MSELoss] - Training Loss: 0.0688, Test Loss: 0.0562\n",
      "Epoch 35 [MSELoss] - Training Loss: 0.0665, Test Loss: 0.0552\n",
      "Epoch 36 [MSELoss] - Training Loss: 0.0661, Test Loss: 0.0543\n",
      "Epoch 37 [MSELoss] - Training Loss: 0.0651, Test Loss: 0.0533\n",
      "Epoch 38 [MSELoss] - Training Loss: 0.0636, Test Loss: 0.0523\n",
      "Epoch 39 [MSELoss] - Training Loss: 0.0640, Test Loss: 0.0512\n",
      "Epoch 40 [MSELoss] - Training Loss: 0.0627, Test Loss: 0.0501\n",
      "Epoch 41 [MSELoss] - Training Loss: 0.0615, Test Loss: 0.0491\n",
      "Epoch 42 [MSELoss] - Training Loss: 0.0605, Test Loss: 0.0481\n",
      "Epoch 43 [MSELoss] - Training Loss: 0.0585, Test Loss: 0.0471\n",
      "Epoch 44 [MSELoss] - Training Loss: 0.0589, Test Loss: 0.0463\n",
      "Epoch 45 [MSELoss] - Training Loss: 0.0581, Test Loss: 0.0453\n",
      "Epoch 46 [MSELoss] - Training Loss: 0.0571, Test Loss: 0.0444\n",
      "Epoch 47 [MSELoss] - Training Loss: 0.0550, Test Loss: 0.0435\n",
      "Epoch 48 [MSELoss] - Training Loss: 0.0540, Test Loss: 0.0426\n",
      "Epoch 49 [MSELoss] - Training Loss: 0.0538, Test Loss: 0.0417\n",
      "Epoch 50 [MSELoss] - Training Loss: 0.0538, Test Loss: 0.0408\n",
      "Epoch 1 [SmoothL1Loss] - Training Loss: 0.2205, Test Loss: 0.2203\n",
      "Epoch 2 [SmoothL1Loss] - Training Loss: 0.2176, Test Loss: 0.2158\n",
      "Epoch 3 [SmoothL1Loss] - Training Loss: 0.2116, Test Loss: 0.2106\n",
      "Epoch 4 [SmoothL1Loss] - Training Loss: 0.2082, Test Loss: 0.2054\n",
      "Epoch 5 [SmoothL1Loss] - Training Loss: 0.2032, Test Loss: 0.2005\n",
      "Epoch 6 [SmoothL1Loss] - Training Loss: 0.1975, Test Loss: 0.1958\n",
      "Epoch 7 [SmoothL1Loss] - Training Loss: 0.1916, Test Loss: 0.1914\n",
      "Epoch 8 [SmoothL1Loss] - Training Loss: 0.1893, Test Loss: 0.1872\n",
      "Epoch 9 [SmoothL1Loss] - Training Loss: 0.1847, Test Loss: 0.1833\n",
      "Epoch 10 [SmoothL1Loss] - Training Loss: 0.1808, Test Loss: 0.1796\n",
      "Epoch 11 [SmoothL1Loss] - Training Loss: 0.1770, Test Loss: 0.1760\n",
      "Epoch 12 [SmoothL1Loss] - Training Loss: 0.1744, Test Loss: 0.1727\n",
      "Epoch 13 [SmoothL1Loss] - Training Loss: 0.1708, Test Loss: 0.1694\n",
      "Epoch 14 [SmoothL1Loss] - Training Loss: 0.1669, Test Loss: 0.1662\n",
      "Epoch 15 [SmoothL1Loss] - Training Loss: 0.1637, Test Loss: 0.1629\n",
      "Epoch 16 [SmoothL1Loss] - Training Loss: 0.1598, Test Loss: 0.1596\n",
      "Epoch 17 [SmoothL1Loss] - Training Loss: 0.1566, Test Loss: 0.1562\n",
      "Epoch 18 [SmoothL1Loss] - Training Loss: 0.1524, Test Loss: 0.1528\n",
      "Epoch 19 [SmoothL1Loss] - Training Loss: 0.1493, Test Loss: 0.1493\n",
      "Epoch 20 [SmoothL1Loss] - Training Loss: 0.1453, Test Loss: 0.1459\n",
      "Epoch 21 [SmoothL1Loss] - Training Loss: 0.1423, Test Loss: 0.1425\n",
      "Epoch 22 [SmoothL1Loss] - Training Loss: 0.1393, Test Loss: 0.1391\n",
      "Epoch 23 [SmoothL1Loss] - Training Loss: 0.1358, Test Loss: 0.1358\n",
      "Epoch 24 [SmoothL1Loss] - Training Loss: 0.1327, Test Loss: 0.1327\n",
      "Epoch 25 [SmoothL1Loss] - Training Loss: 0.1288, Test Loss: 0.1298\n",
      "Epoch 26 [SmoothL1Loss] - Training Loss: 0.1269, Test Loss: 0.1271\n",
      "Epoch 27 [SmoothL1Loss] - Training Loss: 0.1225, Test Loss: 0.1245\n",
      "Epoch 28 [SmoothL1Loss] - Training Loss: 0.1213, Test Loss: 0.1222\n",
      "Epoch 29 [SmoothL1Loss] - Training Loss: 0.1200, Test Loss: 0.1200\n",
      "Epoch 30 [SmoothL1Loss] - Training Loss: 0.1168, Test Loss: 0.1180\n",
      "Epoch 31 [SmoothL1Loss] - Training Loss: 0.1145, Test Loss: 0.1161\n",
      "Epoch 32 [SmoothL1Loss] - Training Loss: 0.1134, Test Loss: 0.1143\n",
      "Epoch 33 [SmoothL1Loss] - Training Loss: 0.1116, Test Loss: 0.1126\n",
      "Epoch 34 [SmoothL1Loss] - Training Loss: 0.1102, Test Loss: 0.1110\n",
      "Epoch 35 [SmoothL1Loss] - Training Loss: 0.1082, Test Loss: 0.1094\n",
      "Epoch 36 [SmoothL1Loss] - Training Loss: 0.1073, Test Loss: 0.1078\n",
      "Epoch 37 [SmoothL1Loss] - Training Loss: 0.1057, Test Loss: 0.1061\n",
      "Epoch 38 [SmoothL1Loss] - Training Loss: 0.1035, Test Loss: 0.1045\n",
      "Epoch 39 [SmoothL1Loss] - Training Loss: 0.1027, Test Loss: 0.1029\n",
      "Epoch 40 [SmoothL1Loss] - Training Loss: 0.1018, Test Loss: 0.1012\n",
      "Epoch 41 [SmoothL1Loss] - Training Loss: 0.0998, Test Loss: 0.0995\n",
      "Epoch 42 [SmoothL1Loss] - Training Loss: 0.0981, Test Loss: 0.0978\n",
      "Epoch 43 [SmoothL1Loss] - Training Loss: 0.0968, Test Loss: 0.0960\n",
      "Epoch 44 [SmoothL1Loss] - Training Loss: 0.0954, Test Loss: 0.0942\n",
      "Epoch 45 [SmoothL1Loss] - Training Loss: 0.0934, Test Loss: 0.0923\n",
      "Epoch 46 [SmoothL1Loss] - Training Loss: 0.0915, Test Loss: 0.0905\n",
      "Epoch 47 [SmoothL1Loss] - Training Loss: 0.0901, Test Loss: 0.0886\n",
      "Epoch 48 [SmoothL1Loss] - Training Loss: 0.0875, Test Loss: 0.0868\n",
      "Epoch 49 [SmoothL1Loss] - Training Loss: 0.0864, Test Loss: 0.0849\n",
      "Epoch 50 [SmoothL1Loss] - Training Loss: 0.0845, Test Loss: 0.0830\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "model = SimpleFeedForwardNN()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_functions = {\n",
    "    \"CrossEntropy\": nn.CrossEntropyLoss(),\n",
    "    \"MSELoss\": nn.MSELoss(),\n",
    "    \"SmoothL1Loss\": nn.SmoothL1Loss()\n",
    "}\n",
    "\n",
    "def evaluate_model(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if name in [\"MSELoss\", \"SmoothL1Loss\"]:\n",
    "                targets = torch.nn.functional.one_hot(targets, 3).float()\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "\n",
    "for name, criterion in loss_functions.items():\n",
    "    model.apply(lambda m: m.reset_parameters() if type(m) == nn.Linear else None)\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if name in [\"MSELoss\", \"SmoothL1Loss\"]:\n",
    "                targets = torch.nn.functional.one_hot(targets, 3).float()\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        test_loss = evaluate_model(model, criterion, test_loader)\n",
    "        print(f\"Epoch {epoch+1} [{name}] - Training Loss: {total_loss / len(train_loader):.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD\n",
      "Epoch 1 - Training Loss: 0.9960, Test Loss: 0.8535, Time: 0.12 seconds\n",
      "Epoch 2 - Training Loss: 0.7526, Test Loss: 0.6448, Time: 0.13 seconds\n",
      "Epoch 3 - Training Loss: 0.6057, Test Loss: 0.5336, Time: 0.12 seconds\n",
      "Epoch 4 - Training Loss: 0.5322, Test Loss: 0.4758, Time: 0.13 seconds\n",
      "Epoch 5 - Training Loss: 0.4879, Test Loss: 0.4363, Time: 0.13 seconds\n",
      "Epoch 6 - Training Loss: 0.4546, Test Loss: 0.4050, Time: 0.12 seconds\n",
      "Epoch 7 - Training Loss: 0.4261, Test Loss: 0.3775, Time: 0.14 seconds\n",
      "Epoch 8 - Training Loss: 0.3984, Test Loss: 0.3569, Time: 0.15 seconds\n",
      "Epoch 9 - Training Loss: 0.3777, Test Loss: 0.3303, Time: 0.13 seconds\n",
      "Epoch 10 - Training Loss: 0.3540, Test Loss: 0.3043, Time: 0.13 seconds\n",
      "Epoch 11 - Training Loss: 0.3308, Test Loss: 0.2718, Time: 0.14 seconds\n",
      "Epoch 12 - Training Loss: 0.2999, Test Loss: 0.2569, Time: 0.13 seconds\n",
      "Epoch 13 - Training Loss: 0.2884, Test Loss: 0.2257, Time: 0.13 seconds\n",
      "Epoch 14 - Training Loss: 0.2621, Test Loss: 0.2045, Time: 0.17 seconds\n",
      "Epoch 15 - Training Loss: 0.2434, Test Loss: 0.1869, Time: 0.16 seconds\n",
      "Epoch 16 - Training Loss: 0.2204, Test Loss: 0.1878, Time: 0.17 seconds\n",
      "Epoch 17 - Training Loss: 0.2047, Test Loss: 0.1770, Time: 0.16 seconds\n",
      "Epoch 18 - Training Loss: 0.1918, Test Loss: 0.1375, Time: 0.18 seconds\n",
      "Epoch 19 - Training Loss: 0.1722, Test Loss: 0.1253, Time: 0.14 seconds\n",
      "Epoch 20 - Training Loss: 0.1569, Test Loss: 0.1144, Time: 0.13 seconds\n",
      "Epoch 21 - Training Loss: 0.1416, Test Loss: 0.1056, Time: 0.12 seconds\n",
      "Epoch 22 - Training Loss: 0.1296, Test Loss: 0.0985, Time: 0.12 seconds\n",
      "Epoch 23 - Training Loss: 0.1163, Test Loss: 0.0901, Time: 0.11 seconds\n",
      "Epoch 24 - Training Loss: 0.1102, Test Loss: 0.0921, Time: 0.14 seconds\n",
      "Epoch 25 - Training Loss: 0.1031, Test Loss: 0.0742, Time: 0.14 seconds\n",
      "Epoch 26 - Training Loss: 0.1019, Test Loss: 0.0854, Time: 0.13 seconds\n",
      "Epoch 27 - Training Loss: 0.1002, Test Loss: 0.0711, Time: 0.12 seconds\n",
      "Epoch 28 - Training Loss: 0.0823, Test Loss: 0.0958, Time: 0.13 seconds\n",
      "Epoch 29 - Training Loss: 0.0822, Test Loss: 0.0601, Time: 0.14 seconds\n",
      "Epoch 30 - Training Loss: 0.0899, Test Loss: 0.0648, Time: 0.13 seconds\n",
      "Epoch 31 - Training Loss: 0.0838, Test Loss: 0.0759, Time: 0.11 seconds\n",
      "Epoch 32 - Training Loss: 0.0834, Test Loss: 0.0567, Time: 0.13 seconds\n",
      "Epoch 33 - Training Loss: 0.0636, Test Loss: 0.0876, Time: 0.12 seconds\n",
      "Epoch 34 - Training Loss: 0.0862, Test Loss: 0.0495, Time: 0.15 seconds\n",
      "Epoch 35 - Training Loss: 0.0797, Test Loss: 0.0533, Time: 0.11 seconds\n",
      "Epoch 36 - Training Loss: 0.0713, Test Loss: 0.0457, Time: 0.11 seconds\n",
      "Epoch 37 - Training Loss: 0.0725, Test Loss: 0.0496, Time: 0.11 seconds\n",
      "Epoch 38 - Training Loss: 0.0736, Test Loss: 0.0558, Time: 0.12 seconds\n",
      "Epoch 39 - Training Loss: 0.0733, Test Loss: 0.0443, Time: 0.11 seconds\n",
      "Epoch 40 - Training Loss: 0.0763, Test Loss: 0.0435, Time: 0.11 seconds\n",
      "Epoch 41 - Training Loss: 0.0716, Test Loss: 0.0486, Time: 0.12 seconds\n",
      "Epoch 42 - Training Loss: 0.0716, Test Loss: 0.0405, Time: 0.13 seconds\n",
      "Epoch 43 - Training Loss: 0.0660, Test Loss: 0.0508, Time: 0.10 seconds\n",
      "Epoch 44 - Training Loss: 0.0684, Test Loss: 0.0400, Time: 0.10 seconds\n",
      "Epoch 45 - Training Loss: 0.0713, Test Loss: 0.0423, Time: 0.12 seconds\n",
      "Epoch 46 - Training Loss: 0.0701, Test Loss: 0.0503, Time: 0.11 seconds\n",
      "Epoch 47 - Training Loss: 0.0717, Test Loss: 0.0385, Time: 0.10 seconds\n",
      "Epoch 48 - Training Loss: 0.0614, Test Loss: 0.0606, Time: 0.09 seconds\n",
      "Epoch 49 - Training Loss: 0.0508, Test Loss: 0.0514, Time: 0.13 seconds\n",
      "Epoch 50 - Training Loss: 0.0671, Test Loss: 0.0345, Time: 0.11 seconds\n",
      "Training with Batch GD\n",
      "Epoch 1 - Training Loss: 1.1365, Test Loss: 1.1175, Time: 0.00 seconds\n",
      "Epoch 2 - Training Loss: 1.1356, Test Loss: 1.1166, Time: 0.00 seconds\n",
      "Epoch 3 - Training Loss: 1.1348, Test Loss: 1.1158, Time: 0.00 seconds\n",
      "Epoch 4 - Training Loss: 1.1339, Test Loss: 1.1149, Time: 0.00 seconds\n",
      "Epoch 5 - Training Loss: 1.1330, Test Loss: 1.1140, Time: 0.01 seconds\n",
      "Epoch 6 - Training Loss: 1.1321, Test Loss: 1.1132, Time: 0.00 seconds\n",
      "Epoch 7 - Training Loss: 1.1313, Test Loss: 1.1123, Time: 0.00 seconds\n",
      "Epoch 8 - Training Loss: 1.1304, Test Loss: 1.1115, Time: 0.00 seconds\n",
      "Epoch 9 - Training Loss: 1.1296, Test Loss: 1.1106, Time: 0.01 seconds\n",
      "Epoch 10 - Training Loss: 1.1287, Test Loss: 1.1098, Time: 0.00 seconds\n",
      "Epoch 11 - Training Loss: 1.1279, Test Loss: 1.1089, Time: 0.01 seconds\n",
      "Epoch 12 - Training Loss: 1.1270, Test Loss: 1.1081, Time: 0.01 seconds\n",
      "Epoch 13 - Training Loss: 1.1262, Test Loss: 1.1073, Time: 0.00 seconds\n",
      "Epoch 14 - Training Loss: 1.1254, Test Loss: 1.1065, Time: 0.01 seconds\n",
      "Epoch 15 - Training Loss: 1.1246, Test Loss: 1.1057, Time: 0.00 seconds\n",
      "Epoch 16 - Training Loss: 1.1237, Test Loss: 1.1049, Time: 0.01 seconds\n",
      "Epoch 17 - Training Loss: 1.1229, Test Loss: 1.1040, Time: 0.01 seconds\n",
      "Epoch 18 - Training Loss: 1.1221, Test Loss: 1.1032, Time: 0.00 seconds\n",
      "Epoch 19 - Training Loss: 1.1213, Test Loss: 1.1025, Time: 0.00 seconds\n",
      "Epoch 20 - Training Loss: 1.1205, Test Loss: 1.1017, Time: 0.00 seconds\n",
      "Epoch 21 - Training Loss: 1.1197, Test Loss: 1.1009, Time: 0.00 seconds\n",
      "Epoch 22 - Training Loss: 1.1189, Test Loss: 1.1001, Time: 0.00 seconds\n",
      "Epoch 23 - Training Loss: 1.1182, Test Loss: 1.0993, Time: 0.00 seconds\n",
      "Epoch 24 - Training Loss: 1.1174, Test Loss: 1.0985, Time: 0.01 seconds\n",
      "Epoch 25 - Training Loss: 1.1166, Test Loss: 1.0978, Time: 0.00 seconds\n",
      "Epoch 26 - Training Loss: 1.1158, Test Loss: 1.0970, Time: 0.00 seconds\n",
      "Epoch 27 - Training Loss: 1.1151, Test Loss: 1.0962, Time: 0.00 seconds\n",
      "Epoch 28 - Training Loss: 1.1143, Test Loss: 1.0955, Time: 0.00 seconds\n",
      "Epoch 29 - Training Loss: 1.1135, Test Loss: 1.0947, Time: 0.01 seconds\n",
      "Epoch 30 - Training Loss: 1.1128, Test Loss: 1.0940, Time: 0.01 seconds\n",
      "Epoch 31 - Training Loss: 1.1120, Test Loss: 1.0932, Time: 0.00 seconds\n",
      "Epoch 32 - Training Loss: 1.1113, Test Loss: 1.0925, Time: 0.00 seconds\n",
      "Epoch 33 - Training Loss: 1.1105, Test Loss: 1.0917, Time: 0.01 seconds\n",
      "Epoch 34 - Training Loss: 1.1098, Test Loss: 1.0910, Time: 0.01 seconds\n",
      "Epoch 35 - Training Loss: 1.1090, Test Loss: 1.0903, Time: 0.00 seconds\n",
      "Epoch 36 - Training Loss: 1.1083, Test Loss: 1.0895, Time: 0.00 seconds\n",
      "Epoch 37 - Training Loss: 1.1075, Test Loss: 1.0888, Time: 0.01 seconds\n",
      "Epoch 38 - Training Loss: 1.1068, Test Loss: 1.0881, Time: 0.00 seconds\n",
      "Epoch 39 - Training Loss: 1.1061, Test Loss: 1.0874, Time: 0.00 seconds\n",
      "Epoch 40 - Training Loss: 1.1054, Test Loss: 1.0866, Time: 0.00 seconds\n",
      "Epoch 41 - Training Loss: 1.1046, Test Loss: 1.0859, Time: 0.01 seconds\n",
      "Epoch 42 - Training Loss: 1.1039, Test Loss: 1.0852, Time: 0.00 seconds\n",
      "Epoch 43 - Training Loss: 1.1032, Test Loss: 1.0845, Time: 0.01 seconds\n",
      "Epoch 44 - Training Loss: 1.1025, Test Loss: 1.0838, Time: 0.01 seconds\n",
      "Epoch 45 - Training Loss: 1.1018, Test Loss: 1.0831, Time: 0.00 seconds\n",
      "Epoch 46 - Training Loss: 1.1011, Test Loss: 1.0824, Time: 0.00 seconds\n",
      "Epoch 47 - Training Loss: 1.1004, Test Loss: 1.0816, Time: 0.00 seconds\n",
      "Epoch 48 - Training Loss: 1.0996, Test Loss: 1.0809, Time: 0.01 seconds\n",
      "Epoch 49 - Training Loss: 1.0989, Test Loss: 1.0802, Time: 0.00 seconds\n",
      "Epoch 50 - Training Loss: 1.0982, Test Loss: 1.0795, Time: 0.01 seconds\n",
      "Training with Mini-Batch GD\n",
      "Epoch 1 - Training Loss: 1.1884, Test Loss: 1.1899, Time: 0.01 seconds\n",
      "Epoch 2 - Training Loss: 1.1807, Test Loss: 1.1814, Time: 0.01 seconds\n",
      "Epoch 3 - Training Loss: 1.1708, Test Loss: 1.1732, Time: 0.01 seconds\n",
      "Epoch 4 - Training Loss: 1.1630, Test Loss: 1.1654, Time: 0.01 seconds\n",
      "Epoch 5 - Training Loss: 1.1576, Test Loss: 1.1581, Time: 0.01 seconds\n",
      "Epoch 6 - Training Loss: 1.1534, Test Loss: 1.1510, Time: 0.01 seconds\n",
      "Epoch 7 - Training Loss: 1.1467, Test Loss: 1.1441, Time: 0.01 seconds\n",
      "Epoch 8 - Training Loss: 1.1397, Test Loss: 1.1375, Time: 0.01 seconds\n",
      "Epoch 9 - Training Loss: 1.1322, Test Loss: 1.1312, Time: 0.00 seconds\n",
      "Epoch 10 - Training Loss: 1.1326, Test Loss: 1.1250, Time: 0.01 seconds\n",
      "Epoch 11 - Training Loss: 1.1242, Test Loss: 1.1189, Time: 0.01 seconds\n",
      "Epoch 12 - Training Loss: 1.1220, Test Loss: 1.1129, Time: 0.01 seconds\n",
      "Epoch 13 - Training Loss: 1.1119, Test Loss: 1.1069, Time: 0.01 seconds\n",
      "Epoch 14 - Training Loss: 1.1074, Test Loss: 1.1012, Time: 0.01 seconds\n",
      "Epoch 15 - Training Loss: 1.1028, Test Loss: 1.0952, Time: 0.01 seconds\n",
      "Epoch 16 - Training Loss: 1.0923, Test Loss: 1.0893, Time: 0.01 seconds\n",
      "Epoch 17 - Training Loss: 1.0947, Test Loss: 1.0832, Time: 0.01 seconds\n",
      "Epoch 18 - Training Loss: 1.0910, Test Loss: 1.0771, Time: 0.01 seconds\n",
      "Epoch 19 - Training Loss: 1.0772, Test Loss: 1.0715, Time: 0.01 seconds\n",
      "Epoch 20 - Training Loss: 1.0748, Test Loss: 1.0660, Time: 0.01 seconds\n",
      "Epoch 21 - Training Loss: 1.0693, Test Loss: 1.0609, Time: 0.01 seconds\n",
      "Epoch 22 - Training Loss: 1.0655, Test Loss: 1.0559, Time: 0.00 seconds\n",
      "Epoch 23 - Training Loss: 1.0598, Test Loss: 1.0508, Time: 0.01 seconds\n",
      "Epoch 24 - Training Loss: 1.0577, Test Loss: 1.0459, Time: 0.01 seconds\n",
      "Epoch 25 - Training Loss: 1.0502, Test Loss: 1.0408, Time: 0.01 seconds\n",
      "Epoch 26 - Training Loss: 1.0508, Test Loss: 1.0360, Time: 0.01 seconds\n",
      "Epoch 27 - Training Loss: 1.0486, Test Loss: 1.0310, Time: 0.01 seconds\n",
      "Epoch 28 - Training Loss: 1.0361, Test Loss: 1.0264, Time: 0.00 seconds\n",
      "Epoch 29 - Training Loss: 1.0331, Test Loss: 1.0217, Time: 0.01 seconds\n",
      "Epoch 30 - Training Loss: 1.0307, Test Loss: 1.0169, Time: 0.01 seconds\n",
      "Epoch 31 - Training Loss: 1.0263, Test Loss: 1.0122, Time: 0.01 seconds\n",
      "Epoch 32 - Training Loss: 1.0179, Test Loss: 1.0074, Time: 0.01 seconds\n",
      "Epoch 33 - Training Loss: 1.0170, Test Loss: 1.0023, Time: 0.01 seconds\n",
      "Epoch 34 - Training Loss: 1.0127, Test Loss: 0.9974, Time: 0.01 seconds\n",
      "Epoch 35 - Training Loss: 1.0062, Test Loss: 0.9924, Time: 0.01 seconds\n",
      "Epoch 36 - Training Loss: 1.0002, Test Loss: 0.9874, Time: 0.00 seconds\n",
      "Epoch 37 - Training Loss: 0.9992, Test Loss: 0.9823, Time: 0.01 seconds\n",
      "Epoch 38 - Training Loss: 0.9902, Test Loss: 0.9771, Time: 0.01 seconds\n",
      "Epoch 39 - Training Loss: 0.9863, Test Loss: 0.9718, Time: 0.01 seconds\n",
      "Epoch 40 - Training Loss: 0.9806, Test Loss: 0.9665, Time: 0.01 seconds\n",
      "Epoch 41 - Training Loss: 0.9769, Test Loss: 0.9612, Time: 0.01 seconds\n",
      "Epoch 42 - Training Loss: 0.9695, Test Loss: 0.9556, Time: 0.00 seconds\n",
      "Epoch 43 - Training Loss: 0.9672, Test Loss: 0.9501, Time: 0.01 seconds\n",
      "Epoch 44 - Training Loss: 0.9644, Test Loss: 0.9444, Time: 0.01 seconds\n",
      "Epoch 45 - Training Loss: 0.9544, Test Loss: 0.9384, Time: 0.00 seconds\n",
      "Epoch 46 - Training Loss: 0.9519, Test Loss: 0.9324, Time: 0.01 seconds\n",
      "Epoch 47 - Training Loss: 0.9447, Test Loss: 0.9263, Time: 0.01 seconds\n",
      "Epoch 48 - Training Loss: 0.9426, Test Loss: 0.9201, Time: 0.00 seconds\n",
      "Epoch 49 - Training Loss: 0.9331, Test Loss: 0.9138, Time: 0.00 seconds\n",
      "Epoch 50 - Training Loss: 0.9309, Test Loss: 0.9075, Time: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_sizes = {\n",
    "    \"SGD\": 1,\n",
    "    \"Batch GD\": len(train_data),\n",
    "    \"Mini-Batch GD\": 32 \n",
    "}\n",
    "\n",
    "models = {name: SimpleFeedForwardNN() for name in batch_sizes.keys()}\n",
    "optimizers = {name: optim.SGD(model.parameters(), lr=0.01) for name, model in models.items()}\n",
    "\n",
    "for name, batch_size in batch_sizes.items():\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = models[name]\n",
    "    optimizer = optimizers[name]\n",
    "    \n",
    "    print(f\"Training with {name}\")\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        \n",
    "        test_loss = evaluate_model(model, nn.CrossEntropyLoss(), test_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} - Training Loss: {total_loss / len(train_loader):.4f}, Test Loss: {test_loss:.4f}, Time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
